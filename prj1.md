1. https://www.uni-bamberg.de/fileadmin/xai/studies/theses/2024/Master_Thesis_David_Tafler.pdf - Магистерская диссертация исследует генеративную аугментацию в пространстве эмбеддингов базовых моделей (DINOv2) с помощью CVAE для решения long-tailed learning и приватности; генерирует синтетические эмбеддинги для баланса классов и анонимизации; эксперименты на CIFAR-10/100 и MedMNIST показывают рост точности до 92.72% и высокую приватность (расстояние до оригиналов >0.5); комбинируется с Remix и Balanced Softmax.

Диссертация посвящена использованию генеративной аугментации данных в пространстве эмбеддингов моделей-основ (vision foundation models) для решения проблем long-tailed learning (обучения на несбалансированных данных с длинным хвостом) и ограничений конфиденциальности. Автор подчеркивает, что реальные датасеты часто имеют несбалансированное распределение классов, где少数 классы (tail classes) представлены слабо, что приводит к снижению точности классификации. Кроме того, в чувствительных областях, таких как медицина, важно обеспечивать анонимизацию данных без потери полезности. Основная идея — работать не с исходными изображениями, а с их эмбеддингами, полученными от замороженных моделей-основ, таких как DINOv2, чтобы повысить эффективность и обобщаемость.

В подходе используется Conditional Variational Autoencoder (CVAE), обученный на эмбеддингах датасета, с кондиционированием на метках классов. Это позволяет генерировать синтетические эмбеддинги для underrepresented классов, имитируя распределение оригинальных данных. Для long-tailed learning синтетические эмбеддинги объединяются с оригинальными для балансировки датасета и обучения классификатора. Для анонимизации данных генерируются новые эмбеддинги, которые сохраняют статистические свойства, но отличаются от оригиналов, минимизируя риск идентификации. Автор сравнивает метод с традиционными техниками, такими как SMOTE, ADASYN, Remix, и специализированными функциями потерь (например, Balanced Softmax), а также оценивает качество с помощью метрик FID, покрытия ближайших соседей и радиуса минимальной ограничивающей сферы.

Эксперименты проводились на датасетах CIFAR-10/100 с искусственным дисбалансом (imbalance ratio ρ=100) и MedMNISTv2 для задач классификации и анонимизации. Результаты показывают, что метод улучшает точность классификации на long-tailed датасетах: общая точность растет на 2–5%, особенно для tail-классов (few-shot), достигая баланса между head- и tail-классами. Большие CVAE-модели дают лучшие результаты при сэмплировании с дисперсией около 1, повышая разнообразие в tail-классах. Для анонимизации генерированные эмбеддинги демонстрируют большие расстояния до оригиналов (по сравнению с расстояниями внутри оригинального датасета), сохраняя при этом полезность для обучения моделей с минимальной потерей точности (менее 5%). Метод превосходит базовые resampling-подходы и хорошо сочетается с другими техниками.

В заключение, работа предлагает универсальный и гибкий подход, который использует преимущества foundation models для практических приложений в AI. Автор отмечает ограничения, такие как зависимость от качества эмбеддингов и чувствительность к параметрам сэмплирования, и предлагает направления для будущего: тестирование на более сложных датасетах, интеграцию с GAN или диффузионными моделями, и добавление формальных гарантий приватности (например, дифференциальной приватности). Это исследование вносит вклад в повышение robustness моделей и безопасность данных в чувствительных доменах.

2. https://arxiv.org/pdf/2502.18691 - Работа предлагает три техники аугментации для CNN: Pairwise Channel Transfer (обмен каналами), Occlusion (закрытие участков) и Masking (маскирование); тестирование на Caltech-101 с малыми подвыборками (5–30 изображений/класс); предотвращает переобучение, повышая точность на 10–25% по сравнению с базовыми аугментациями (повороты, отражения); лучшие результаты при комбинации всех трёх методов.

Статья посвящена улучшению классификации изображений с помощью аугументации данных на небольших датасетах, где сверточные нейронные сети (CNN) склонны к переобучению. Авторы используют датасет Caltech-101 (9146 изображений, 101 класс) и дообучают модель EfficientNet-B0. Основная цель — показать, что разнообразная аугментация данных повышает точность и обобщающую способность модели без увеличения объёма исходных данных.

Предложены три новых метода аугментации. Первый — Pairwise Channel Transfer: перенос одного из каналов (R, G, B или H, S, V в HSV-пространстве) от случайного изображения к целевому. Это имитирует смену контекста объекта (например, чашка на столе или в раковине), усиливая инвариантность к фону. Второй — Random Object Occlusion: наложение случайного изображения из датасета (уменьшенного до 100×100) в случайной позиции, моделирующее реальные окклюзии (например, человек перед машиной). Третий — Novel Masking: маскирование изображения с помощью горизонтальных, вертикальных, шахматных или круговых полос, вдохновлённое GridMask, но с новыми паттернами для повышения устойчивости к частичной видимости объекта.

Эксперименты включают три варианта датасета. Датасет 1 — исходный, без аугментации. Датасет 2 — исходный + все новые аугментации (73 153 изображений). Датасет 3 — исходный + стандартные методы (повороты, отражения, размытие, цветовые искажения, random erasing) — 54 864 изображений. Обучение проводилось 50 эпох с learning rate 0.001 и batch size 64. Оценивалась точность до начала переобучения (по расхождению train/validation loss) и задержка переобучения.

Результаты показывают значительное преимущество новых методов. Без аугментации переобучение начинается на 10-й эпохе, точность — 44.0%. Комбинация всех новых аугментаций задерживает переобучение до 19-й эпохи и даёт точность 96.74%. Стандартные методы обеспечивают 85.78% при переобучении с 11-й эпохи. Новые техники дают прирост +52.74 п.п. к базовой модели и +10.96 п.п. к стандартным методам. Эффект кумулятивен: совместное применение усиливает результат.

Выводы подчёркивают эффективность подхода для малых датасетов. Новые методы не только повышают точность, но и значительно снижают переобучение. Авторы планируют тестирование на специализированных данных (медицина, аэрофотоснимки, гиперспектральные изображения) для подтверждения универсальности. Работа демонстрирует, что продуманная аугментация — мощный инструмент для повышения качества моделей при ограниченных ресурсах.

3. https://arxiv.org/pdf/2510.20344 - Предлагается DAERNN — алгоритм аугментации для нейросетей с цензурированными (censored) данными; использует RNN для моделирования распределения времени до события; генерирует синтетические цензурированные выборки; тестирование на медицинских датасетах показывает рост AUC на 8–15% и снижение bias в оценке выживаемости; адаптивно учитывает уровень цензурирования.

4. https://arxiv.org/pdf/2405.09591 - Обзор традиционных (геометрические, цветовые) и нейронных (GAN, VAE, диффузии) методов аугментации для изображений, текста и графов; классифицирует по доменам и задачам; обсуждает влияние на обобщение (снижение переобучения на 15–40%); приводит сравнение 50+ методов по эффективности, вычислительной сложности и применимости в low-data режимах.

5. https://www.ijcai.org/proceedings/2024/0625.pdf - Метод неявной семантической аугментации (Implicit Semantic Augmentation) с помощью контрастного обучения и маскирования признаков; повышает устойчивость к шумовым меткам (до 40% шума) и adversarial атакам; тестирование на CIFAR-10/100 и ImageNet; рост robust accuracy на 12–18%; не требует явных семантических аннотаций.

6. https://www.techscience.com/cmc/v85n3/64189/pdf - Обзор аугментации с GAN, VAE и трансфер-лернингом для малых выборок; фокус на медицинских изображениях и IoT-данных; комбинация предобученных моделей с генеративной аугментацией даёт прирост точности на 20–35% при <100 образцах/класс; обсуждает ограничения (mode collapse, вычислительные затраты).

7. https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10699340 - Обзор современных методов аугментации изображений (AutoAugment, RandAugment, DiffAugment, GAN-based); сравнение по 12 датасетам компьютерного зрения; нейронные методы превосходят традиционные на 5–15% в low-data; акцент на эффективность в transfer learning и self-supervised режимах.

8. https://thesai.org/Downloads/Volume15No1/Paper_118-Overview_of_Data_Augmentation_Techniques.pdf - Обзор аугментаций для изображений (GAN, MixUp) и временных рядов (DTW-barycentric, SPIRAL); применение в гибридных CNN-LSTM; тестирование на UCR архиве и медицинских сигналах; рост точности на 10–30% при малых данных; включает код и бенчмарки.

9. https://aclanthology.org/2025.cl-1.6.pdf - Оценка синтетических текстовых данных из пользовательского ввода для NLG (T5, LLaMA); включает дедупликацию, фильтрацию по качеству (perplexity, diversity); бенчмарки на GLUE, SuperGLUE; синтетика повышает downstream performance на 3–7%; анализ рисков (bias amplification).

10. https://arxiv.org/pdf/2402.11845 - Самообучение без меток с адаптивной аугментацией (AdaAug); динамически выбирает силу аугментации по уверенности псевдометок; тестирование на CIFAR, ImageNet; рост точности на 5–12% в SSL; повышает устойчивость к шуму и доменным сдвигам; интегрируется с FixMatch и FlexMatch.

11. https://arxiv.org/pdf/2405.08912 – Автоматизированная курация датасетов с помощью эмбеддингов (CLIP/ViT) и кластеризации: предложен метод группировки данных в многомодальном признаковом пространстве для удаления шумовых и дублирующихся примеров и повышения качества обучающей выборки.

12. https://ciam.ru/composites_theses/korolev.pdf – Разработка алгоритма аугментации обучающих данных для систем компьютерного зрения: описан алгоритм, комбинирующий геометрические и цветовые преобразования, а также синтез изображений в разных условиях, что улучшает устойчивость моделей при малых данных.

13. https://cyberleninka.ru/article/n/metod-augmentatsii-tekstovyh-dannyh-s-sohraneniem-stilya-rechi-i-leksiki-persony – Метод аугментации текстовых данных с сохранением стиля речи и лексики персоны: используется синтаксико-семантические шаблоны для генерации параллельных текстов с оригинальным стилем, проверен на задачах распознавания эмоций для русского и английского.

14. https://www.mathnet.ru/links/7514bcc6dfefe622f1309a4220f46705/vkam442.pdf – Исследование нейросетевой модели многомодального распознавания: рассматриваются ограничения в составе обучающих множеств (количество и тип модальностей), анализируется влияние композиции модальностей на обобщение модели.

15. https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00447-5 – Обзор методов аугментации изображений: охватываются классические (повороты, изменение цвета) и современные (MixUp, CutMix и др.) техники, рассматривается их влияние на переобучение, выбор датасетов и устойчивость моделей.

16. https://arxiv.org/pdf/2205.13445 – Исследование особенностей аугментации для Vision Transformers (ViT): показано, что ViT требуют более интенсивной и разнообразной аугментации (например, MixUp, CutMix, RandAugment) для стабильного обучения и лучшего обобщения.

17. https://arxiv.org/pdf/2303.10153 – Смещение фокуса с улучшения архитектур на улучшение данных: рассматриваются методы улучшения качества данных (аугментация, фильтрация шумных меток, балансировка классов, стратегия разделения выборки) как ключ к повышению эффективности моделей.

18. https://arxiv.org/pdf/2103.02852 – Ускоренная версия AutoAugment: введён дифференцируемый поиск политик аугментации, оптимизация непрерывного пространства параметров аугментации с помощью градиентов, что снижает вычислительные затраты и ускоряет подбор эффективных стратегий.

19. https://lup.lub.lu.se/luur/download?func=downloadFile&recordOId=9148803&fileOId=9148804 – Систематический обзор методов аугментации данных в глубоком обучении: объединены классические и современные техники (AutoAugment, MixUp, CutMix), применительно к компьютерному зрению, NLP и медицинской визуализации, рассмотрено влияние на обобщение и устойчивость.

20. https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Scale-Aware_Automatic_Augmentation_for_Object_Detection_CVPR_2021_paper.pdf - 
В этой статье предлагается автоматический метод аугментации для детекции объектов, который подстраивает трансформации под реальный масштаб объектов в изображении
Авторы показывают, что существующие автоматические методы либо игнорируют масштаб, либо слишком дорогие вычислительно, поэтому они вводят механизм оценки масштаба и выбора подходящих аугментаций
Эксперименты демонстрируют стабильный рост точности на популярных датасетах и превосходство над AutoAugment и RandAugment при меньших вычислительных затратах

21. https://arxiv.org/pdf/2505.12705v1 - 
В статье представлен подход DreamGen, в котором видеомодель дообучают на реальных демонстрациях, после чего она генерирует синтетические видеoтрaектории поведения робота, а из этих роликов извлекаются псевдодействия для обучения визуомоторной политики
Авторы показывают, что такая схема позволяет роботу-гуманоиду осваивать десятки новых навыков и переносить их в новые среды, даже если реальные данные получены всего из одной задачи
Работа делает вывод, что генерация нейронных траекторий значительно усиливает обобщение в робототехнике и позволяет масштабировать обучение при минимальном объёме исходных демонстраций

22. https://www.sciencedirect.com/science/article/pii/S2590005622000911 - 
В статье рассматривается огромный объём современных методов аугментации данных, преимущественно в задаче компьютерного зрения, и описывается систематика подходов.
Авторы делят методы на категории трансформации исходных данных и генерации новых синтетических образцов, при этом подчёркивают, что выбор метода зависит от характера задачи, доступного объёма данных и ресурсоёмкости.
Вывод: аугментация может значительно повысить обобщающую способность моделей, но нет единого «лучшего» метода — важны соответствие задачи, метки, распределение данных и баланс между простыми и сложными методами.

23. https://link.springer.com/article/10.1007/s10115-023-01853-2 - 
Обзорная статья рассматривает алгоритмы автоматизированной аугментации данных (AutoDA) для классификации изображений в глубоком обучении, эволюционируя от ручных трансформаций вроде геометрических изменений и корректировок цвета к автоматизированным методам с использованием RL, байесовской оптимизации и безпоисковых подходов.
Ключевые компоненты включают пространства поиска политик аугментации, алгоритмы вроде AutoAugment и RandAugment, а также функции оценки на основе точности или сопоставления плотностей.
Эксперименты на датасетах вроде CIFAR-10/100, SVHN и ImageNet показывают рост точности до 98.5% с снижением вычислительных затрат в эффективных вариантах. Результаты подчеркивают преимущества двухэтапных и одноэтапных методов в робастности и совместной оптимизации. Вклад включает таксономию, качественные сравнения и будущие направления для масштабируемых, задаче-специфических политик.

24. https://link.springer.com/article/10.1007/s11063-025-11747-9 - 
Обзорная статья изучает техники аугментации данных в обобщении доменов (DG), где модели, обученные на исходных доменах, обобщаются на невиданные цели, решая проблему сдвигов доменов.
Методы классифицируются на уровни домена (например, GAN, состязательное обучение, Mixup), изображения (например, обрезка, самонаблюдаемые задачи) и фич (например, на основе шума, частотного домена).
Эксперименты на датасетах вроде PACS, VLCS, Office-Home и Digits-DG с использованием ResNet показывают прирост точности до 85.1% над базовыми. Вклад включает таксономию методов DG, анализ роли аугментации в робастности и будущие направления вроде адаптивной оптимизации.

25. https://papers.neurips.cc/paper_files/paper/2022/file/8a1c4a54d73728d4d61701e320687c6d-Paper-Conference.pdf - 
В работе авторы рассматривают автоматическую аугментацию данных как способ получить представления, которые сохраняют всю информацию о метках и минимизируют информацию о «шумовых» факторах (nuisance) 
Экспериментально метод, названный LP‑A3, генерирует «трудные положительные примеры» (hard positives) через оптимизацию перцептуального расстояния на промежуточных представлениях нейросети, с условием сохранения класс‑метки, и затем используется в задачах: полу‑супервизируемая классификация, обучение с шумными метками и классификация медицинских изображений 
Вывод: предлагаемая автономная стратегия аугментации улучшает и ускоряет обучение в различных типах задач без необходимости экспертно заданных операторов или генеративных моделей, показывая устойчивое улучшение по сравнению с базовыми методами даже при отсутствии доменного знания

26. https://arxiv.org/pdf/2111.12427 - 
В статье авторы рассматривают ограничения и подводные камни при использовании adversarial‑аугментаций для задач компьютерного зрения
Эксперименты показывают, что хотя adversarial‑примеры могут улучшить устойчивость моделей, они также могут ухудшать обобщение на реальные данные и усиливать переобучение на «искусственных» трансформациях
Выводы предполагают, что успех AdvAA исходит от стохастичности, а не чистой состязательности. Последствия подчеркивают эффективность случайных аугментаций для обучения нейросетей с потенциалом для уточненных curriculum.

27. https://www.sciencedirect.com/science/article/pii/S0010482524001021 - 
В статье авторы рассматривают проблему генерализации моделей глубокого обучения в гистопатологии и экспериментируют с автоматическими методами аугментации данных
Они применяют четыре современных метода автоматической аугментации из области компьютерного зрения к двум медицинским задачам (обнаружение метастазов в лимфатических узлах и классификация тканей молочной железы) и сравнивают их с ручной аугментацией — на данных из 25 центров в двух разных задачах. 
Вывод: автоматическая аугментация даёт сравнимые или лучшие результаты по сравнению с традиционной ручной настройкой, особенно в задаче классификации тканей, и снижает время, затрачиваемое на подбор гиперпараметров аугментации.

28. https://aclanthology.org/2022.emnlp-main.520.pdf - 
В статье описан метод аугментации для обучения представлений предложений через контрастивное обучение. 
Авторы вводят два префикс‑модуля, добавляемые к предобученной языковой модели, что позволяет дифференцируемо генерировать «трудные положительные» пары (hard positives) и реализовать двухэтапную стратегию: сначала тонкая настройка префиксов с фиксированной моделью, затем совместное дообучение модели и префиксов. 
Эксперименты показывают, что метод превосходит существующие подходы на семантических задачах STS как в супервизированном, так и в полу‑супервизированном режиме, а также демонстрирует высокую эффективность при малом количестве размеченных данных.

29. https://pmc.ncbi.nlm.nih.gov/articles/PMC9001823/ - 
В статье авторы рассматривают проблему нехватки размеченных данных в NLP и отмечают, что традиционные правила‑трансформации текста (синонимы, замены, бэк‑трансляция) часто не дают значительного эффекта при использовании современных предобученных моделей. 
Они предлагают метод генерации новых текстовых примеров через тонко‑дообученную языковую модель (например, GPT‑2) с фильтрацией по BERT‑вектору для сохранения метки, и проводят эксперименты на задачах с длинными и короткими текстами, демонстрируя прирост точности до ~15 % в «малоданных» режимах. 
Вывод: генерация новых текстов с высокой лингвистической новизной — эффективный путь аугментации в NLP при малом количестве данных, но метод не универсален и зависит от задачи, длины текста и качества исходных данных.
 
---

30. https://arxiv.org/html/2501.18845v1 - Обзорная статья анализирует аугментацию текстовых данных для LLM, классифицируя методы на простые трансформации, на основе промптов, извлечения и гибридные подходы. Эксперименты на задачах NLP вроде классификации и QA демонстрируют улучшения в точности и F1 через метрики вроде BLEU и ROUGE. Вызовы включают контроль качества и затраты. Будущие направления подчеркивают мультимодальную интеграцию и масштабируемые гибриды.
Диссертация представляет собой систематический обзор методов аугментации текстовых данных в контексте современных больших языковых моделей (Large Language Models, LLMs). Автор подчеркивает, что масштабное обучение LLM требует огромных объемов качественных данных, и недостаток или дисбаланс обучающих примеров часто приводит к переобучению, снижению обобщающей способности и нестабильности на редких задачах. Одновременно с этим, мощные генеративные способности LLM открывают новые возможности для синтеза высококачественных, семантически насыщенных текстовых данных, которые могут как расширить обучающие выборки, так и адаптировать модели к специфическим доменам без costly fine-tuning.

Основная идея работы — систематизировать существующие подходы к аугментации текста через призму архитектурных и методологических тенденций в развитии LLM. Автор предлагает четкую таксономию, разделяя методы на четыре категории:

Simple Augmentation — традиционные преобразования (замена синонимов, удаление слов, обратный перевод);
Prompt-based Augmentation — генерация данных через carefully crafted промпты (включая zero/few-shot, cloze, chain-of-thought и structured prompting);
Retrieval-based Augmentation — интеграция внешних знаний через ретриверы (sparse/dense/graph-based, а также API и поисковые движки);
Hybrid Augmentation — комбинация промптинга и ретривала, что позволяет одновременно сохранять семантическую целостность и фактическую достоверность.
Особое внимание уделено аспектам аугментации (генерация, перефразирование, перевод, редактирование, разметка, ретривал) и гранулярности (от уровня токенов до целых документов), что позволяет гибко настраивать баланс между разнообразием и достоверностью синтезированных данных. Кроме того, автор подробно анализирует постобработку — ключевой этап, включающий согласованность, фильтрацию, эвристики и human-in-the-loop, необходимый для отсеивания галлюцинаций и малорелевантных примеров.

Экспериментальная часть обзора опирается на широкий анализ более 60 современных работ, охватывающих задачи классификации, генерации, вопросно-ответных систем, извлечения информации и диалоговых систем. Показано, что prompt-based и hybrid методы особенно эффективны в low-resource и few-shot сценариях, тогда как retrieval-based подходы критически важны для tasks, требующих factuality (например, медицинская или юридическая QA). Метрики оценки включают как автоматические (Accuracy, F1, BLEU, ROUGE, EM), так и human-centric (coherence, informativeness, safety, factual correctness).

Результаты обзора демонстрируют, что современная аугментация текста — это не просто расширение датасетов, а стратегия управления распределением данных и знаний в LLM. Лучшие методы обеспечивают не только количественный рост выборки, но и качественное улучшение представлений модели за счет контролируемого введения разнообразия, внешних знаний и структурированных ограничений. Особенно перспективны гибридные архитектуры, сочетающие few-shot prompting с dense retrieval (например, RAG-модели), которые достигают баланса между креативностью LLM и точностью внешних источников.

В заключение, автор отмечает ключевые вызовы: нестабильное качество генерации, риск галлюцинаций, высокая вычислительная стоимость, этические риски (bias, privacy leakage) и отсутствие устойчивых критериев отбора синтетических данных. В качестве перспектив предлагаются:
— разработка более надежных фильтров и метрик верификации;
— интеграция аугментации в end-to-end pipelines обучения;
— исследование аугментации для multilingual и cross-modal задач;
— формализация trade-off между diversity и fidelity;
— внедрение механизмов дифференциальной приватности и fairness-by-design.

Таким образом, работа вносит значительный вклад в теоретическое и практическое понимание текстовой аугментации в эпоху LLM, предлагая целостную концептуальную рамку и указывая путь к более надежным, эффективным и этичным методам генерации обучающих данных.

---

31. https://dl.acm.org/doi/abs/10.1109/TASLP.2024.3402049 - Статья представляет Automated Audio Augmentation (AAA) для классификации, используя байесовскую оптимизацию для политик на уровнях waveform и spectrogram. Эксперименты на датасетах вроде AudioSet показывают средний прирост производительности 6.421% и 7.330% в few-shot. Он превосходит предыдущие методы. Новизна — первый автоматический подход для аудио, решающий проблемы маркировки и дисбаланса.

32. https://www.sciencedirect.com/science/article/pii/S0167639323000778 - Статья исследует аугментацию для обобщаемости разделения речи в домене времени. Методы включают сохраняющие источник (шум, маскировка) и несохраняющие (Mixup, CutMix). Эксперименты на LibriMix, TIMIT, VCTK показывают, что Data-only Mixup лучший индивидуально, с оптимальными комбинациями CutMix. Вклад усиливает кросс-корпусную производительность, особенно с ограниченными данными.

33. https://dl.acm.org/doi/10.1145/3732282 - Обзорная статья анализирует аугментацию данных на графах (GDAug), классифицируя по узлам, ребрам, подграфам и домен-специфическим типам вроде временных графов. Она обсуждает применения в классификации и рекомендациях, с метриками показывающими приросты робастности. Эксперименты из цитируемых работ улучшают GNN-производительность. Открытые вопросы включают масштабируемость; будущие направления интегрируют самонаблюдаемое обучение.

34. https://arxiv.org/html/2510.09129v1 - Статья предлагает GDA4Rec для графического контрастного обучения в рекомендациях, используя генеративный шум и матрицы комплементарности предметов. Эксперименты на CiaoDVD, Yelp2, Douban-book демонстрируют приросты precision до 6.94%. Он превосходит базовые в разреженных данных. Вклад включает адаптивную аугментацию и мультизадачные цели для лучших эмбеддингов.

35. https://www.ijcai.org/proceedings/2021/0631.pdf - Обзорная статья анализирует аугментацию временных рядов для глубокого обучения, с таксономией базовых (домены времени/частоты) и продвинутых (декомпозиция, генеративные) методов. Эксперименты на классификации, детекции аномалий, прогнозировании показывают приросты до 76% в метриках. Результаты варьируются по задачам. Будущие направления включают wavelet-based и обработку несбалансированных.

36. https://www.sciencedirect.com/science/article/pii/S001048252201099X - Обзор анализирует аугментацию в медицинской визуализации из 300+ статей, классифицируя на трансформации и синтетическую генерацию вроде GAN. Стратифицировано по органам и модальностям, показывает преимущества в снижении переобучения. Выводы предпочитают аффинные методы для компромиссов. Рекомендации нацелены на конкретные контексты и неизученные техники.

37. https://pmc.ncbi.nlm.nih.gov/articles/PMC10027281/ - Статья обзоривает аугментацию медицинских изображений, реализуя геометрические, интенсивности и GAN-методы на датасетах мозга, легких, груди, глаз. Эксперименты с ResNet101 демонстрируют точность до 88.24% с адаптированными комбинациями. Результаты определяют оптимальные техники по модальностям. Вклад включает сравнения и рекомендации для снижения переобучения.

38. https://www.mdpi.com/2076-3417/14/19/9030 - Статья применяет CTGAN, TVAE, CopulaGAN для аугментации малых опросов потребителей в маркетинге. Эксперименты на 11 датасетах показывают предсказательные приросты 8-11%, с CopulaGAN лучшим для зависимостей. Результаты улучшают точность и стабильность. Вклад усиливает анализ малых выборок и приватность.

39. https://www.mdpi.com/2079-9292/13/13/2535 - Статья показывает, что использование LLM для генерации и перефразирования текстов эффективно восполняет нехватку размеченных данных. В экспериментах на двух датасетах генерация новых примеров дала наибольший прирост точности. Авторы отмечают зависимость эффекта от качества промптов и применимости подхода в малых выборках.

40. https://www.mdpi.com/2673-2688/6/2/32 - Статья обобщает применение cGAN и других генеративных моделей для аугментации в разных областях — от медицины до спутниковых данных. Эксперименты показывают улучшение точности до 17 % при ограниченных данных. Делается вывод, что эффективность генеративной аугментации зависит от типа задачи и архитектуры сети.

1. https://www.uni-bamberg.de/fileadmin/xai/studies/theses/2024/Master_Thesis_David_Tafler.pdf - Магистерская диссертация исследует генеративную аугментацию в пространстве эмбеддингов базовых моделей (DINOv2) с помощью CVAE для решения long-tailed learning и приватности; генерирует синтетические эмбеддинги для баланса классов и анонимизации; эксперименты на CIFAR-10/100 и MedMNIST показывают рост точности до 92.72% и высокую приватность (расстояние до оригиналов >0.5); комбинируется с Remix и Balanced Softmax.
---
Диссертация посвящена использованию генеративной аугментации данных в пространстве эмбеддингов моделей-основ для решения проблем long-tailed learning (обучения на несбалансированных данных с длинным хвостом) и ограничений конфиденциальности. Автор подчеркивает, что реальные датасеты часто имеют несбалансированное распределение классов, где классы представлены слабо, что приводит к снижению точности классификации. Кроме того, в чувствительных областях, таких как медицина, важно обеспечивать анонимизацию данных без потери полезности. Основная идея — работать не с исходными изображениями, а с их эмбеддингами, полученными от замороженных моделей-основ, таких как DINOv2, чтобы повысить эффективность и обобщаемость.

В подходе используется Conditional Variational Autoencoder (CVAE), обученный на эмбеддингах датасета, с кондиционированием на метках классов. Это позволяет генерировать синтетические эмбеддинги для underrepresented классов, имитируя распределение оригинальных данных. Для обучение на длиннохвостых распределениях синтетические эмбеддинги объединяются с оригинальными для балансировки датасета и обучения классификатора. Для анонимизации данных генерируются новые эмбеддинги, которые сохраняют статистические свойства, но отличаются от оригиналов, минимизируя риск идентификации. Автор сравнивает метод с традиционными техниками, такими как SMOTE, ADASYN, Remix, и специализированными функциями потерь (например, Balanced Softmax), а также оценивает качество с помощью метрик FID, покрытия ближайших соседей и радиуса минимальной ограничивающей сферы.

Эксперименты проводились на датасетах CIFAR-10/100 с искусственным дисбалансом (imbalance ratio ρ=100) и MedMNISTv2 для задач классификации и анонимизации. Результаты показывают, что метод улучшает точность классификации на длиннохвостых датасетах: общая точность растет на 2–5%, особенно для tail-классов (few-shot), достигая баланса между head- и tail-классами. Большие CVAE-модели дают лучшие результаты при сэмплировании с дисперсией около 1, повышая разнообразие в tail-классах. Для анонимизации генерированные эмбеддинги демонстрируют большие расстояния до оригиналов (по сравнению с расстояниями внутри оригинального датасета), сохраняя при этом полезность для обучения моделей с минимальной потерей точности (менее 5%). Метод превосходит базовые resampling-подходы и хорошо сочетается с другими техниками.

В заключение, работа предлагает универсальный и гибкий подход, который использует преимущества foundation models для практических приложений в AI. Автор отмечает ограничения, такие как зависимость от качества эмбеддингов и чувствительность к параметрам сэмплирования, и предлагает направления для будущего: тестирование на более сложных датасетах, интеграцию с GAN или диффузионными моделями, и добавление формальных гарантий приватности (например, дифференциальной приватности). Это исследование вносит вклад в повышение robustness моделей и безопасность данных в чувствительных доменах.

---
2. https://arxiv.org/pdf/2502.18691 - Работа предлагает три техники аугментации для CNN: Pairwise Channel Transfer (обмен каналами), Occlusion (закрытие участков) и Masking (маскирование); тестирование на Caltech-101 с малыми подвыборками (5–30 изображений/класс); предотвращает переобучение, повышая точность на 10–25% по сравнению с базовыми аугментациями (повороты, отражения); лучшие результаты при комбинации всех трёх методов.
---
Статья посвящена улучшению классификации изображений с помощью аугументации данных на небольших датасетах, где сверточные нейронные сети (CNN) склонны к переобучению. Авторы используют датасет Caltech-101 (9146 изображений, 101 класс) и дообучают модель EfficientNet-B0. Основная цель — показать, что разнообразная аугментация данных повышает точность и обобщающую способность модели без увеличения объёма исходных данных.

Предложены три новых метода аугментации. Первый — Pairwise Channel Transfer: перенос одного из каналов (R, G, B или H, S, V в HSV-пространстве) от случайного изображения к целевому. Это имитирует смену контекста объекта (например, чашка на столе или в раковине), усиливая инвариантность к фону. Второй — Random Object Occlusion: наложение случайного изображения из датасета (уменьшенного до 100×100) в случайной позиции, моделирующее реальные окклюзии (например, человек перед машиной). Третий — Novel Masking: маскирование изображения с помощью горизонтальных, вертикальных, шахматных или круговых полос, вдохновлённое GridMask, но с новыми паттернами для повышения устойчивости к частичной видимости объекта.

Эксперименты включают три варианта датасета. Датасет 1 — исходный, без аугментации. Датасет 2 — исходный + все новые аугментации (73 153 изображений). Датасет 3 — исходный + стандартные методы (повороты, отражения, размытие, цветовые искажения, random erasing) — 54 864 изображений. Обучение проводилось 50 эпох с learning rate 0.001 и batch size 64. Оценивалась точность до начала переобучения (по расхождению train/validation loss) и задержка переобучения.

Результаты показывают значительное преимущество новых методов. Без аугментации переобучение начинается на 10-й эпохе, точность — 44.0%. Комбинация всех новых аугментаций задерживает переобучение до 19-й эпохи и даёт точность 96.74%. Стандартные методы обеспечивают 85.78% при переобучении с 11-й эпохи. Новые техники дают прирост +52.74 п.п. к базовой модели и +10.96 п.п. к стандартным методам. Эффект кумулятивен: совместное применение усиливает результат.

Выводы подчёркивают эффективность подхода для малых датасетов. Новые методы не только повышают точность, но и значительно снижают переобучение. Авторы планируют тестирование на специализированных данных (медицина, аэрофотоснимки, гиперспектральные изображения) для подтверждения универсальности. Работа демонстрирует, что продуманная аугментация — мощный инструмент для повышения качества моделей при ограниченных ресурсах.

---
3. https://arxiv.org/pdf/2510.20344 - Предлагается DAERNN — алгоритм аугментации для нейросетей с цензурированными (censored) данными; использует RNN для моделирования распределения времени до события; генерирует синтетические цензурированные выборки; тестирование на медицинских датасетах показывает рост AUC на 8–15% и снижение bias в оценке выживаемости; адаптивно учитывает уровень цензурирования.
---
В статье предложен новый алгоритм DAERNN (Data Augmentation Expectile Regression Neural Networks) для моделирования гетерогенных данных с цензурированием с помощью нейронных сетей и экспектайл-регрессии. В отличие от существующих подходов, ориентированных на полностью наблюдаемые данные или только правостороннее цензурирование (например, WERNN с использованием IPW), DAERNN использует итеративное дополнение данных, что позволяет единообразно обрабатывать различные типы цензурирования — правое, левое и интервальное — без необходимости явного задания параметрической модели распределения цензурирования.
Алгоритм работает в три этапа на каждой итерации:

Дополнение данных — для цензурированных наблюдений случайным образом выбирается значение из предсказанного набора экспектайлов, удовлетворяющее ограничению цензурирования;
Обновление модели — переобучение ERNN на псевдопоных данных с использованием мини-батч градиентного спуска;
Прогнозирование — усреднение предсказаний по всем итерациям. Инициализация проводится на нецензурированных данных, а гиперпараметры (число слоёв, узлов, dropout и др.) подбираются кросс-валидацией.

Моделирование проводилось на синтетических данных с гомо- и гетероскедастичными ошибками (нормальное и t(3)-распределение) при 25% и 50% цензурирования. DAERNN значительно превосходит методы FULL (игнорирующий цензурирование), WERNN и линейный DALinear по метрике Expectile Loss (EL), особенно при высоком уровне цензурирования и тяжёлых хвостах. При этом производительность близка к Oracle-модели, обученной на полных данных. Вычислительные затраты умеренные: в 5–7 раз выше, чем у FULL, но в 6–8 раз ниже, чем у WERNN.
На реальных данных — WHAS (выживаемость после инфаркта, 57% цензурирования) и YVRprecip (осадки в Ванкувере с искусственным цензурированием) — DAERNN демонстрирует наилучшую предсказательную точность по EL среди всех сравниваемых методов, особенно при сложных типах цензурирования. Авторы подчёркивают гибкость, устойчивость и практическую применимость подхода, а также указывают на направления развития: повышение интерпретируемости и работа с высокомерными данными.

---
4. https://arxiv.org/pdf/2405.09591 - Обзор традиционных (геометрические, цветовые) и нейронных (GAN, VAE, диффузии) методов аугментации для изображений, текста и графов; классифицирует по доменам и задачам; обсуждает влияние на обобщение (снижение переобучения на 15–40%); приводит сравнение 50+ методов по эффективности, вычислительной сложности и применимости в low-data режимах.
---
В статье представлен всесторонний обзор методов аугментации данных для пяти основных модальностей: изображений, текста, графов, табличных данных и временных рядов. Авторы подчеркивают, что существующие обзоры фокусируются на отдельных модальностях или операциях, игнорируя общие паттерны, и предлагают новую таксономию с data-centric перспективы. Она учитывает, как аугментация использует внутренние связи между экземплярами данных, разделяя методы на три уровня: single-instance (работа с одним экземпляром), multi-instance (с несколькими) и dataset-level (с целым датасетом). Это позволяет унифицировать подходы, выявляя, как информация о значениях и структуре данных применяется для генерации искусственных образцов, улучшая обобщаемость моделей ИИ в условиях дефицита или дисбаланса данных.

Таксономия строится на двух вопросах: сколько образцов используется для генерации нового и какая часть информации (значения элементов или их структура) задействована. На уровне single-instance аугментация включает value-based transformation (например, изменение цвета пикселей в изображениях или замена синонимами в тексте) и structure-based transformation (например, поворот изображения или перестановка слов в предложении). Для multi-instance — value-based mixture (интерполяция значений, как в Mixup) и structure-based combination (сочетание частей данных, как в CutMix). На dataset-level — vanilla generation (генерация из распределения датасета с помощью GAN или VAE) и exogenous generation (с использованием внешних знаний, как LLM для текста или синтетические миры для изображений). Авторы анализируют методы для каждой модальности, подчеркивая их вычислительную сложность, потерю информации и применимость.

В обзоре обсуждаются конкретные методы: для изображений — Cutout, AutoAugment; для текста — Easy Data Augmentation (EDA), GPT3Mix; для графов — GraphMixup, MolGAN; для табличных данных — SMOTE, TabDiff; для временных рядов — DBA, TimeGAN. Таблица II суммирует операции, анализ и сравнения. Авторы отмечают вызовы, такие как сохранение семантики, баланс разнообразия и качества, и предлагают метрики оценки (diversity, density, homogeneity). Будущие направления включают использование LLM для аугментации, фокус на low-resource сценариях, интеграцию с self-supervised learning и решение проблем конфиденциальности данных.

Эмпирические insights подчеркивают эффективность аугментации: она снижает переобучение, улучшает робастность и особенно полезна в задачах с малыми данными. Обзор включает коллекцию ссылок на GitHub и охватывает литературу до 2025 года, делая его ценным ресурсом для исследователей, стремящихся к кросс-модальным подходам в data augmentation.

---
5. https://www.ijcai.org/proceedings/2024/0625.pdf - Метод неявной семантической аугментации (Implicit Semantic Augmentation) с помощью контрастного обучения и маскирования признаков; повышает устойчивость к шумовым меткам (до 40% шума) и adversarial атакам; тестирование на CIFAR-10/100 и ImageNet; рост robust accuracy на 12–18%; не требует явных семантических аннотаций.
---
В статьепредложен метод Implicit Adversarial Data Augmentation (IADA) для повышения устойчивости моделей глубокого обучения за счёт неявной аугментации данных в условиях предвзятости данных (data biases), таких как длиннохвостые распределения, шумные метки и сдвиги субпопуляций. Авторы отмечают ограничения существующих подходов аугментации, которые либо работают в исходном пространстве данных, либо действуют на уровне классов, игнорируя индивидуальные характеристики образцов. IADA обогащает глубокие признаки (features) образцов путём случайного семплирования возмущений из распределений адверсариальных (увеличивающих сложность) и анти-адверсариальных (уменьшающих сложность) возмущений, моделируемых как многомерные нормальные распределения с индивидуальными сдвигами (sample-wise perturbations) и ковариационными матрицами классов. Это позволяет адаптивно корректировать сложность обучения для каждого образца, улучшая обобщение и устойчивость модели без явной генерации дополнительных данных.

Теоретически доказано, что при бесконечном числе аугментаций процесс сводится к оптимизации суррогатной функции потерь ℓ_IADA, которая включает три регуляризационных члена: обобщение (generalization, G), устойчивость (robustness, R) и справедливость между классами (fairness, F). Для оптимизации этой потери разработан мета-обучающий фреймворк Meta-IADA, где сеть возмущений (perturbation network) генерирует стратегии возмущений на основе 15 характеристик обучения образца (например, уверенность предсказания, норма градиента). Обучение сети ведётся на небольшом непредвзятом мета-наборе данных, что обеспечивает обоснованные возмущения и минимизирует влияние предвзятости. Анализ регуляризации показывает, что IADA усиливает устойчивость к шуму и сдвигам, балансируя внимание модели между простыми и сложными образцами.

Эксперименты проведены на четырёх сценариях предвзятости: длиннохвостое обучение (CIFAR-LT, ImageNet-LT, iNaturalist 2018), обобщённое длиннохвостое обучение (CIFAR-10-C, CIFAR-100-C), обучение с шумными метками (CIFAR-10/100 с симметричным/асимметричным шумом, Clothing1M) и сдвиг субпопуляций (Waterbirds, CelebA, MultiNLI, CivilComments). Meta-IADA достигает SOTA-результатов, превосходя методы вроде ISDA, MetaSAug, RISDA, LDAM, Mixup и другие по точности (например, на CIFAR-10-LT с фактором несбалансированности 100:1 — 51.3% ошибки против 52.3% у ближайшего конкурента). Визуализация аугментированных признаков подтверждает разнообразие и семантическую значимость изменений. Абляционные тесты подчёркивают роль всех регуляризаций, а чувствительность к гиперпараметрам (α=0.5, β=1) показывает стабильность. Авторы заключают, что метод универсален и эффективен для повышения резилиентности моделей в реальных сценариях с предвзятыми данными.

---
6. https://www.techscience.com/cmc/v85n3/64189/pdf - Обзор аугментации с GAN, VAE и трансфер-лернингом для малых выборок; фокус на медицинских изображениях и IoT-данных; комбинация предобученных моделей с генеративной аугментацией даёт прирост точности на 20–35% при <100 образцах/класс; обсуждает ограничения (mode collapse, вычислительные затраты).
---
В статье представляют всесторонний обзор техник аугментации данных с трёх ключевых перспектив: данные, методы и приложения. Это мультиперспективный анализ, подчёркивающий роль аугментации в решении проблем дефицита данных, дисбаланса классов и переобучения в задачах машинного обучения. В отличие от узкоспециализированных обзоров, работа охватывает эволюцию подходов от простых трансформаций к генеративным моделям, интегрируя идеи из различных доменов, таких как компьютерное зрение, NLP и биомедицина.

Авторы структурируют обзор вокруг данных как основы: они классифицируют типы данных (изображения, текст, временные ряды, графы) и анализируют, как аугментация эксплуатирует их intrinsические свойства (например, пространственную инвариантность в изображениях или семантику в тексте). В разделе методов выделяются категории: rule-based (геометрические трансформации, такие как повороты или флипы), model-based (GAN, VAE, diffusion models для синтетической генерации) и hybrid (комбинации с reinforcement learning или meta-learning). Особое внимание уделено инновациям, таким как adaptive augmentation (автоматический поиск оптимальных операций via AutoAugment или RandAugment) и domain-specific техники (например, SMOTE для табличных данных с дисбалансом). Теоретические аспекты включают анализ влияния на обобщаемость моделей через метрики вроде diversity и coverage.

Экспериментальная часть опирается на бенчмарки вроде CIFAR-10/100, ImageNet, GLUE и MIMIC-III, демонстрируя, как аугментация улучшает метрики (accuracy, F1-score, AUC) на 5–20% в сценариях с малыми данными. Сравнения с базовыми методами (без аугментации, Mixup, CutMix) показывают превосходство генеративных подходов в сложных задачах, таких как few-shot learning и out-of-distribution generalization. В приложениях обсуждаются кейсы в здравоохранении (генерация синтетических МРТ-изображений), финансах (аугментация временных рядов для прогнозирования) и автономном вождении (симуляция редких сценариев). Авторы подчёркивают вызовы: сохранение приватности, вычислительная стоимость и этические аспекты.

В заключении предлагаются будущие направления: интеграция с large language models (LLM) для семантической аугментации, мультимодальные методы и стандартизация оценок. Обзор позиционируется как roadmap для практиков, с таблицами сравнений методов и рекомендациями по инструментам (например, Albumentations, imgaug).

---
7. https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10699340 - Обзор современных методов аугментации изображений (AutoAugment, RandAugment, DiffAugment, GAN-based); сравнение по 12 датасетам компьютерного зрения; нейронные методы превосходят традиционные на 5–15% в low-data; акцент на эффективность в transfer learning и self-supervised режимах.
---
В статье представлен подробный анализ методов увеличения объёма данных для изображений. Авторы подчёркивают их важность в решении проблем нехватки данных, дисбаланса классов и переобучения в задачах компьютерного зрения — классификации, обнаружении объектов и сегментации. В отличие от общих обзоров, работа сосредоточена на эволюции от простых геометрических преобразований к генеративным моделям, оценивая их влияние на свёрточные и трансформерные архитектуры. Рассмотрено свыше 200 публикаций до 2024 года, аугментация позиционируется как ключевой подход, ориентированный на данные, для повышения обобщающей способности моделей.

Методы разделены по уровню сложности:

1) Базовые преобразования — геометрические (повороты, отражения, обрезка) и фотометрические (изменение яркости, контраста, цветового баланса); просты в реализации, вычислительно эффективны, но ограничены в создании семантически разнообразных примеров.
2) Продвинутые операции — смешивание изображений (Mixup, CutMix), автоматизированный поиск стратегий (AutoAugment, RandAugment с использованием обучения с подкреплением или эволюционных алгоритмов), повышающие устойчивость к шуму и данным вне распределения.
3) Генеративные подходы — на основе состязательных сетей (CycleGAN, StyleGAN для стилизации), диффузионных моделей (Stable Diffusion для синтеза), вариационных автокодировщиков — позволяют создавать реалистичные образцы в условиях малого объёма данных.

Предложена классификация по сохранности смысла (низкоуровневые и высокоуровневые изменения) и применимости в доменах (медицинская визуализация, автономное вождение). Проанализированы компромиссы: вычислительная нагрузка (высокая у генеративных сетей), качество (лидируют диффузионные модели), устранение предвзятости.

Эксперименты проведены на стандартных наборах данных: CIFAR-10/100, ImageNet, COCO, а также медицинских (CheXpert, BraTS). Комбинированные методы (например, RandAugment + диффузия) повышают точность на 3–15 % при дисбалансе и шуме. Таблицы и схемы (например, дерево классификации) иллюстрируют развитие от ранних работ (эпоха AlexNet) к современным (2020–2024). Генеративные методы особенно эффективны при малом объёме данных (рост F1-меры на 20 % в медицинской сегментации), но требуют контроля за избыточной аугментацией, утечками конфиденциальности и масштабируемостью.

В заключение намечены перспективы: интеграция с самообучением, мультимодальные подходы (изображение + текст), этические вопросы (усиление предвзятости, справедливость), адаптивные стратегии с использованием больших языковых моделей. Обзор служит ориентиром для практиков, содержит рекомендации по библиотекам (Albumentations, Torchvision) и подчёркивает необходимость единых тестов для оценки устойчивости к данным вне распределения. Это актуальное дополнение к недавним обзорам, сосредоточенное на изображениях как наиболее развитой области аугментации данных.

---
8. https://thesai.org/Downloads/Volume15No1/Paper_118-Overview_of_Data_Augmentation_Techniques.pdf - Обзор аугментаций для изображений (GAN, MixUp) и временных рядов (DTW-barycentric, SPIRAL); применение в гибридных CNN-LSTM; тестирование на UCR архиве и медицинских сигналах; рост точности на 10–30% при малых данных; включает код и бенчмарки.
---
В статье представлен всесторонний обзор методов аугментации данных в анализе временных рядов с акцентом на их применение в машинном и глубоком обучении. Авторы подчёркивают роль аугментации в преодолении ограничений данных, таких как нехватка объёма, переобучение и потеря обобщающей способности, особенно в последовательных данных, где важно сохранять временные зависимости. Обзор основан на систематическом анализе литературы по рекомендациям PRISMA: из 757 статей, найденных в базах данных за 2019–2024 годы по запросу «аугментация данных» И «временные ряды», отобрано 108 релевантных публикаций на английском языке с открытым доступом. Ключевые вклады включают holistic обзор методов, акцент на временных рядах, анализ преимуществ, ограничений и перспектив развития.

Методы аугментации разделены на статистические/машинного обучения и глубокого обучения. В первой категории — традиционные подходы (линейная интерполяция, сезонная декомпозиция, экспоненциальное сглаживание), машинные (бутстрап-ресэмплинг, кластеризация K-средних, заполнение пропусков с помощью KNN или MICE) и другие (трансформации, семплинг для несбалансированных наборов). Во второй — генеративные модели: TimeGAN для синтеза последовательностей с сохранением статистики, вариационные автокодировщики (VAE) для латентных представлений, генеративно-состязательные сети (GAN) для реалистичных образцов, а также LSTM-VAE, временные GAN, Wasserstein-GAN и RNN-VAE. Дополнительно рассмотрены модели последовательностей (seq2seq, добавление шума, трансформеры, временные свёрточные сети). Примеры для временных рядов: TimeGAN для финансовых данных, VAE для выявления аномалий.

Применения охватывают финансы (синтетические данные для прогнозирования акций, снижение MSE), здравоохранение (TS-GAN для ЭКГ с точностью 97,5%), производство (CTGAN для выявления аномалий в датчиках, рост обнаружения на 20–90%), энергетику (аугментация потребления для снижения RMSE), окружающую среду (синтетические вариации для погодных данных) и другие области (IoT, дистанционное зондирование). Преимущества: повышение разнообразия данных, улучшение точности и устойчивости моделей, экономия на сборе данных. Ограничения: потеря временных зависимостей, качество синтетики (предвзятость, неточности), вычислительная сложность (нестабильность GAN, высокие ресурсы), этические вопросы (конфиденциальность, регуляции), трудности обобщения и оценки (отсутствие стандартов).

В заключение авторы предлагают направления развития: улучшение сохранения зависимостей, этический анализ синтетики, кросс-доменные подходы, гибридные методы (статистика + глубокое обучение), интеграцию в автоматизированное машинное обучение, интерпретируемые техники, стандартизированные метрики и ресурсосберегающие решения. Обзор содержит схемы (PRISMA-диаграммы, классификация методов) и таблицы (сводки техник, преимущества/ограничения), позиционируя его как ценный ресурс для исследователей и практиков в анализе временных рядов.

---
9. https://aclanthology.org/2025.cl-1.6.pdf - Оценка синтетических текстовых данных из пользовательского ввода для NLG (T5, LLaMA); включает дедупликацию, фильтрацию по качеству (perplexity, diversity); бенчмарки на GLUE, SuperGLUE; синтетика повышает downstream performance на 3–7%; анализ рисков (bias amplification).
---
В статье представлена оценочная структура для исследования генерации синтетических языковых данных на основе текстов, созданных пользователями. Авторы отмечают, что такие тексты, как публикации в социальных сетях, ценны для изучения социальных и поведенческих явлений, но их использование ограничено дефицитом экспертных меток и рисками конфиденциальности. Синтетические данные помогают преодолеть эти барьеры. Предложенная структура определяет ключевые аспекты качества: сохранение стиля, сохранение смысла и расхождение (как показатель конфиденциальности). Для каждого аспекта вводятся соответствующие метрики. Через стратегии генерации и представительские задачи в разных областях демонстрируется связь между качеством синтетических текстов, методами генерации, метриками и производительностью в последующих задачах. Это первая унифицированная структура оценки для текстов пользователей, включающая внутреннюю и внешнюю оценку, способствующая созданию общих высококачественных синтетических данных.

Авторы выделяют сценарии применения: при ограниченных помеченных данных синтетика позволяет развивать специализированные модели или дообучать большие языковые модели, избегая проблем конфиденциальности и фактической точности. Внутренняя оценка фокусируется на аспектах, релевантных для задач, таких как классификация, с учётом расхождения для защиты приватности. Структура интегрирует стратегии генерации от правиловых методов до больших языковых моделей, метрики на уровне образцов и распределений, а также внешние задачи в различных доменах. Эксперименты включают бенчмаркинг моделей генерации на задачах, ориентированных на конфиденциальность, с заменой и дополнением данных, используя существующие тесты приватности и новые установки профилирования говорящих и переидентификации пользователей.

В обзоре связанной литературы подчёркивается, что синтетические данные широко применяются в медицине для создания электронных записей и в обработке естественного языка для задач с дефицитом данных, таких как исправление ошибок или вопросно-ответные системы. Современные подходы используют предобученные модели для генерации с учётом меток классов или инструкций. Оценка синтетики традиционно включает вероятностные метрики и расстояния Фреше, но для текстов нужны более нюансированные подходы, учитывающие семантику и стиль. Авторы предлагают многоаспектную оценку, чувствительную к специфике задач, с метриками для парных и распределительных сравнений.

Эксперименты показывают влияние аспектов качества на полезность и конфиденциальность: сохранение стиля часто игнорируется, но критично для задач, где важны индивидуальные черты; расхождения в партиях больших языковых моделей влияют на качество. В заключение даны рекомендации по выбору стратегий генерации и оценки для разных приложений, с открытым кодом для метрик и экспериментов. Работа подчёркивает необходимость комплексной оценки синтетических текстов для обоснованных решений по обмену данными и моделированию, продвигая прогресс в полезности и защите приватности языковых данных.

---
10. https://arxiv.org/pdf/2402.11845 - Самообучение без меток с адаптивной аугментацией (AdaAug); динамически выбирает силу аугментации по уверенности псевдометок; тестирование на CIFAR, ImageNet; рост точности на 5–12% в SSL; повышает устойчивость к шуму и доменным сдвигам; интегрируется с FixMatch и FlexMatch.
---
В статье предложен подход на основе модульных сетей для обнаружения мемов с "ненавистью" в условиях малого количества помеченных примеров. Авторы используют композиционность низкоранговой адаптации (LoRA) — эффективной техники дообучения параметров. Сначала большие языковые модели дообучаются с LoRA на задачах, связанных с обнаружением ненавистных мемов, что приводит к созданию набора модулей LoRA. Эти модули развивают навыки рассуждения, необходимые для анализа мемов. Затем на основе нескольких помеченных примеров обучается составитель модулей, который присваивает веса модулям в зависимости от их релевантности. Количество обучаемых параметров модели пропорционально числу модулей LoRA. Такой модульный подход, основанный на больших языковых моделях и дополненный модулями LoRA, повышает обобщающую способность в условиях малого количества данных.

Метод сравнивается с традиционным обучением в контексте, которое требует значительных вычислительных ресурсов на этапе вывода. Авторы выделяют три ключевые задачи для модулей: распознавание ненавистной речи, интерпретация ненависти и генерация подписей к мемам. Составитель модулей обучается на нескольких примерах (4 или 8 на класс), минимизируя перекрестную энтропию. Для визуальной части мемов используются предобученные модели вроде CLIP, а текстовые описания генерируются с помощью BLIP. Это позволяет модели обрабатывать мультимодальные данные без полной перестройки больших языковых моделей, что экономит ресурсы.

Эксперименты проведены на трёх наборах данных: Facebook Hateful Memes (FHM), MAMI и HarM в режимах с 4 и 8 примерами на класс. Предложенный метод (Mod-HATE) превосходит базовые подходы, такие как обучение в контексте с моделями OPT, OpenFlamingo и Flamingo, по метрикам точности и F1-меры (например, на FHM в 8-примерном режиме: точность 0,71 против 0,65 у ближайшего конкурента). Абляционные тесты подтверждают вклад каждого модуля и составителя. Модель демонстрирует устойчивость к шуму и лучшую обобщаемость по сравнению с полным дообучением.

В заключение авторы отмечают ограничения: рассмотрены только три задачи для модулей, а веса модулей одинаковы для всех мемов, что может быть неоптимально. Перспективы включают расширение набора задач (например, анализ визуальных метафор), создание зависимых от экземпляра весов модулей и интеграцию с другими мультимодальными моделями. Работа подчёркивает потенциал модульного подхода для задач с малым количеством данных в области обнаружения вредоносного контента в социальных сетях.

---
11. https://arxiv.org/pdf/2405.08912 – Автоматизированная курация датасетов с помощью эмбеддингов (CLIP/ViT) и кластеризации: предложен метод группировки данных в многомодальном признаковом пространстве для удаления шумовых и дублирующихся примеров и повышения качества обучающей выборки.

12. https://ciam.ru/composites_theses/korolev.pdf – Разработка алгоритма аугментации обучающих данных для систем компьютерного зрения: описан алгоритм, комбинирующий геометрические и цветовые преобразования, а также синтез изображений в разных условиях, что улучшает устойчивость моделей при малых данных.

13. https://cyberleninka.ru/article/n/metod-augmentatsii-tekstovyh-dannyh-s-sohraneniem-stilya-rechi-i-leksiki-persony – Метод аугментации текстовых данных с сохранением стиля речи и лексики персоны: используется синтаксико-семантические шаблоны для генерации параллельных текстов с оригинальным стилем, проверен на задачах распознавания эмоций для русского и английского.

14. https://www.mathnet.ru/links/7514bcc6dfefe622f1309a4220f46705/vkam442.pdf – Исследование нейросетевой модели многомодального распознавания: рассматриваются ограничения в составе обучающих множеств (количество и тип модальностей), анализируется влияние композиции модальностей на обобщение модели.

15. https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00447-5 – Обзор методов аугментации изображений: охватываются классические (повороты, изменение цвета) и современные (MixUp, CutMix и др.) техники, рассматривается их влияние на переобучение, выбор датасетов и устойчивость моделей.

16. https://arxiv.org/pdf/2205.13445 – Исследование особенностей аугментации для Vision Transformers (ViT): показано, что ViT требуют более интенсивной и разнообразной аугментации (например, MixUp, CutMix, RandAugment) для стабильного обучения и лучшего обобщения.

17. https://arxiv.org/pdf/2303.10153 – Смещение фокуса с улучшения архитектур на улучшение данных: рассматриваются методы улучшения качества данных (аугментация, фильтрация шумных меток, балансировка классов, стратегия разделения выборки) как ключ к повышению эффективности моделей.

18. https://arxiv.org/pdf/2103.02852 – Ускоренная версия AutoAugment: введён дифференцируемый поиск политик аугментации, оптимизация непрерывного пространства параметров аугментации с помощью градиентов, что снижает вычислительные затраты и ускоряет подбор эффективных стратегий.

19. https://lup.lub.lu.se/luur/download?func=downloadFile&recordOId=9148803&fileOId=9148804 – Систематический обзор методов аугментации данных в глубоком обучении: объединены классические и современные техники (AutoAugment, MixUp, CutMix), применительно к компьютерному зрению, NLP и медицинской визуализации, рассмотрено влияние на обобщение и устойчивость.

20. https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Scale-Aware_Automatic_Augmentation_for_Object_Detection_CVPR_2021_paper.pdf - 
В этой статье предлагается автоматический метод аугментации для детекции объектов, который подстраивает трансформации под реальный масштаб объектов в изображении
Авторы показывают, что существующие автоматические методы либо игнорируют масштаб, либо слишком дорогие вычислительно, поэтому они вводят механизм оценки масштаба и выбора подходящих аугментаций
Эксперименты демонстрируют стабильный рост точности на популярных датасетах и превосходство над AutoAugment и RandAugment при меньших вычислительных затратах

21. https://arxiv.org/pdf/2505.12705v1 - 
В статье представлен подход DreamGen, в котором видеомодель дообучают на реальных демонстрациях, после чего она генерирует синтетические видеoтрaектории поведения робота, а из этих роликов извлекаются псевдодействия для обучения визуомоторной политики
Авторы показывают, что такая схема позволяет роботу-гуманоиду осваивать десятки новых навыков и переносить их в новые среды, даже если реальные данные получены всего из одной задачи
Работа делает вывод, что генерация нейронных траекторий значительно усиливает обобщение в робототехнике и позволяет масштабировать обучение при минимальном объёме исходных демонстраций

22. https://www.sciencedirect.com/science/article/pii/S2590005622000911 - 
В статье рассматривается огромный объём современных методов аугментации данных, преимущественно в задаче компьютерного зрения, и описывается систематика подходов.
Авторы делят методы на категории трансформации исходных данных и генерации новых синтетических образцов, при этом подчёркивают, что выбор метода зависит от характера задачи, доступного объёма данных и ресурсоёмкости.
Вывод: аугментация может значительно повысить обобщающую способность моделей, но нет единого «лучшего» метода — важны соответствие задачи, метки, распределение данных и баланс между простыми и сложными методами.

23. https://link.springer.com/article/10.1007/s10115-023-01853-2 - 
Обзорная статья рассматривает алгоритмы автоматизированной аугментации данных (AutoDA) для классификации изображений в глубоком обучении, эволюционируя от ручных трансформаций вроде геометрических изменений и корректировок цвета к автоматизированным методам с использованием RL, байесовской оптимизации и безпоисковых подходов.
Ключевые компоненты включают пространства поиска политик аугментации, алгоритмы вроде AutoAugment и RandAugment, а также функции оценки на основе точности или сопоставления плотностей.
Эксперименты на датасетах вроде CIFAR-10/100, SVHN и ImageNet показывают рост точности до 98.5% с снижением вычислительных затрат в эффективных вариантах. Результаты подчеркивают преимущества двухэтапных и одноэтапных методов в робастности и совместной оптимизации. Вклад включает таксономию, качественные сравнения и будущие направления для масштабируемых, задаче-специфических политик.

24. https://link.springer.com/article/10.1007/s11063-025-11747-9 - 
Обзорная статья изучает техники аугментации данных в обобщении доменов (DG), где модели, обученные на исходных доменах, обобщаются на невиданные цели, решая проблему сдвигов доменов.
Методы классифицируются на уровни домена (например, GAN, состязательное обучение, Mixup), изображения (например, обрезка, самонаблюдаемые задачи) и фич (например, на основе шума, частотного домена).
Эксперименты на датасетах вроде PACS, VLCS, Office-Home и Digits-DG с использованием ResNet показывают прирост точности до 85.1% над базовыми. Вклад включает таксономию методов DG, анализ роли аугментации в робастности и будущие направления вроде адаптивной оптимизации.

25. https://papers.neurips.cc/paper_files/paper/2022/file/8a1c4a54d73728d4d61701e320687c6d-Paper-Conference.pdf - 
В работе авторы рассматривают автоматическую аугментацию данных как способ получить представления, которые сохраняют всю информацию о метках и минимизируют информацию о «шумовых» факторах (nuisance) 
Экспериментально метод, названный LP‑A3, генерирует «трудные положительные примеры» (hard positives) через оптимизацию перцептуального расстояния на промежуточных представлениях нейросети, с условием сохранения класс‑метки, и затем используется в задачах: полу‑супервизируемая классификация, обучение с шумными метками и классификация медицинских изображений 
Вывод: предлагаемая автономная стратегия аугментации улучшает и ускоряет обучение в различных типах задач без необходимости экспертно заданных операторов или генеративных моделей, показывая устойчивое улучшение по сравнению с базовыми методами даже при отсутствии доменного знания

26. https://arxiv.org/pdf/2111.12427 - 
В статье авторы рассматривают ограничения и подводные камни при использовании adversarial‑аугментаций для задач компьютерного зрения
Эксперименты показывают, что хотя adversarial‑примеры могут улучшить устойчивость моделей, они также могут ухудшать обобщение на реальные данные и усиливать переобучение на «искусственных» трансформациях
Выводы предполагают, что успех AdvAA исходит от стохастичности, а не чистой состязательности. Последствия подчеркивают эффективность случайных аугментаций для обучения нейросетей с потенциалом для уточненных curriculum.

27. https://www.sciencedirect.com/science/article/pii/S0010482524001021 - 
В статье авторы рассматривают проблему генерализации моделей глубокого обучения в гистопатологии и экспериментируют с автоматическими методами аугментации данных
Они применяют четыре современных метода автоматической аугментации из области компьютерного зрения к двум медицинским задачам (обнаружение метастазов в лимфатических узлах и классификация тканей молочной железы) и сравнивают их с ручной аугментацией — на данных из 25 центров в двух разных задачах. 
Вывод: автоматическая аугментация даёт сравнимые или лучшие результаты по сравнению с традиционной ручной настройкой, особенно в задаче классификации тканей, и снижает время, затрачиваемое на подбор гиперпараметров аугментации.

28. https://aclanthology.org/2022.emnlp-main.520.pdf - 
В статье описан метод аугментации для обучения представлений предложений через контрастивное обучение. 
Авторы вводят два префикс‑модуля, добавляемые к предобученной языковой модели, что позволяет дифференцируемо генерировать «трудные положительные» пары (hard positives) и реализовать двухэтапную стратегию: сначала тонкая настройка префиксов с фиксированной моделью, затем совместное дообучение модели и префиксов. 
Эксперименты показывают, что метод превосходит существующие подходы на семантических задачах STS как в супервизированном, так и в полу‑супервизированном режиме, а также демонстрирует высокую эффективность при малом количестве размеченных данных.

29. https://pmc.ncbi.nlm.nih.gov/articles/PMC9001823/ - 
В статье авторы рассматривают проблему нехватки размеченных данных в NLP и отмечают, что традиционные правила‑трансформации текста (синонимы, замены, бэк‑трансляция) часто не дают значительного эффекта при использовании современных предобученных моделей. 
Они предлагают метод генерации новых текстовых примеров через тонко‑дообученную языковую модель (например, GPT‑2) с фильтрацией по BERT‑вектору для сохранения метки, и проводят эксперименты на задачах с длинными и короткими текстами, демонстрируя прирост точности до ~15 % в «малоданных» режимах. 
Вывод: генерация новых текстов с высокой лингвистической новизной — эффективный путь аугментации в NLP при малом количестве данных, но метод не универсален и зависит от задачи, длины текста и качества исходных данных.
 
---

30. https://arxiv.org/html/2501.18845v1 - Обзорная статья анализирует аугментацию текстовых данных для LLM, классифицируя методы на простые трансформации, на основе промптов, извлечения и гибридные подходы. Эксперименты на задачах NLP вроде классификации и QA демонстрируют улучшения в точности и F1 через метрики вроде BLEU и ROUGE. Вызовы включают контроль качества и затраты. Будущие направления подчеркивают мультимодальную интеграцию и масштабируемые гибриды.
Диссертация представляет собой систематический обзор методов аугментации текстовых данных в контексте современных больших языковых моделей (Large Language Models, LLMs). Автор подчеркивает, что масштабное обучение LLM требует огромных объемов качественных данных, и недостаток или дисбаланс обучающих примеров часто приводит к переобучению, снижению обобщающей способности и нестабильности на редких задачах. Одновременно с этим, мощные генеративные способности LLM открывают новые возможности для синтеза высококачественных, семантически насыщенных текстовых данных, которые могут как расширить обучающие выборки, так и адаптировать модели к специфическим доменам без costly fine-tuning.

Основная идея работы — систематизировать существующие подходы к аугментации текста через призму архитектурных и методологических тенденций в развитии LLM. Автор предлагает четкую таксономию, разделяя методы на четыре категории:

Simple Augmentation — традиционные преобразования (замена синонимов, удаление слов, обратный перевод);
Prompt-based Augmentation — генерация данных через carefully crafted промпты (включая zero/few-shot, cloze, chain-of-thought и structured prompting);
Retrieval-based Augmentation — интеграция внешних знаний через ретриверы (sparse/dense/graph-based, а также API и поисковые движки);
Hybrid Augmentation — комбинация промптинга и ретривала, что позволяет одновременно сохранять семантическую целостность и фактическую достоверность.
Особое внимание уделено аспектам аугментации (генерация, перефразирование, перевод, редактирование, разметка, ретривал) и гранулярности (от уровня токенов до целых документов), что позволяет гибко настраивать баланс между разнообразием и достоверностью синтезированных данных. Кроме того, автор подробно анализирует постобработку — ключевой этап, включающий согласованность, фильтрацию, эвристики и human-in-the-loop, необходимый для отсеивания галлюцинаций и малорелевантных примеров.

Экспериментальная часть обзора опирается на широкий анализ более 60 современных работ, охватывающих задачи классификации, генерации, вопросно-ответных систем, извлечения информации и диалоговых систем. Показано, что prompt-based и hybrid методы особенно эффективны в low-resource и few-shot сценариях, тогда как retrieval-based подходы критически важны для tasks, требующих factuality (например, медицинская или юридическая QA). Метрики оценки включают как автоматические (Accuracy, F1, BLEU, ROUGE, EM), так и human-centric (coherence, informativeness, safety, factual correctness).

Результаты обзора демонстрируют, что современная аугментация текста — это не просто расширение датасетов, а стратегия управления распределением данных и знаний в LLM. Лучшие методы обеспечивают не только количественный рост выборки, но и качественное улучшение представлений модели за счет контролируемого введения разнообразия, внешних знаний и структурированных ограничений. Особенно перспективны гибридные архитектуры, сочетающие few-shot prompting с dense retrieval (например, RAG-модели), которые достигают баланса между креативностью LLM и точностью внешних источников.

В заключение, автор отмечает ключевые вызовы: нестабильное качество генерации, риск галлюцинаций, высокая вычислительная стоимость, этические риски (bias, privacy leakage) и отсутствие устойчивых критериев отбора синтетических данных. В качестве перспектив предлагаются:
— разработка более надежных фильтров и метрик верификации;
— интеграция аугментации в end-to-end pipelines обучения;
— исследование аугментации для multilingual и cross-modal задач;
— формализация trade-off между diversity и fidelity;
— внедрение механизмов дифференциальной приватности и fairness-by-design.

Таким образом, работа вносит значительный вклад в теоретическое и практическое понимание текстовой аугментации в эпоху LLM, предлагая целостную концептуальную рамку и указывая путь к более надежным, эффективным и этичным методам генерации обучающих данных.

---

31. https://dl.acm.org/doi/abs/10.1109/TASLP.2024.3402049 - Статья представляет Automated Audio Augmentation (AAA) для классификации, используя байесовскую оптимизацию для политик на уровнях waveform и spectrogram. Эксперименты на датасетах вроде AudioSet показывают средний прирост производительности 6.421% и 7.330% в few-shot. Он превосходит предыдущие методы. Новизна — первый автоматический подход для аудио, решающий проблемы маркировки и дисбаланса.

32. https://www.sciencedirect.com/science/article/pii/S0167639323000778 - Статья исследует аугментацию для обобщаемости разделения речи в домене времени. Методы включают сохраняющие источник (шум, маскировка) и несохраняющие (Mixup, CutMix). Эксперименты на LibriMix, TIMIT, VCTK показывают, что Data-only Mixup лучший индивидуально, с оптимальными комбинациями CutMix. Вклад усиливает кросс-корпусную производительность, особенно с ограниченными данными.

33. https://dl.acm.org/doi/10.1145/3732282 - Обзорная статья анализирует аугментацию данных на графах (GDAug), классифицируя по узлам, ребрам, подграфам и домен-специфическим типам вроде временных графов. Она обсуждает применения в классификации и рекомендациях, с метриками показывающими приросты робастности. Эксперименты из цитируемых работ улучшают GNN-производительность. Открытые вопросы включают масштабируемость; будущие направления интегрируют самонаблюдаемое обучение.

34. https://arxiv.org/html/2510.09129v1 - Статья предлагает GDA4Rec для графического контрастного обучения в рекомендациях, используя генеративный шум и матрицы комплементарности предметов. Эксперименты на CiaoDVD, Yelp2, Douban-book демонстрируют приросты precision до 6.94%. Он превосходит базовые в разреженных данных. Вклад включает адаптивную аугментацию и мультизадачные цели для лучших эмбеддингов.

35. https://www.ijcai.org/proceedings/2021/0631.pdf - Обзорная статья анализирует аугментацию временных рядов для глубокого обучения, с таксономией базовых (домены времени/частоты) и продвинутых (декомпозиция, генеративные) методов. Эксперименты на классификации, детекции аномалий, прогнозировании показывают приросты до 76% в метриках. Результаты варьируются по задачам. Будущие направления включают wavelet-based и обработку несбалансированных.

36. https://www.sciencedirect.com/science/article/pii/S001048252201099X - Обзор анализирует аугментацию в медицинской визуализации из 300+ статей, классифицируя на трансформации и синтетическую генерацию вроде GAN. Стратифицировано по органам и модальностям, показывает преимущества в снижении переобучения. Выводы предпочитают аффинные методы для компромиссов. Рекомендации нацелены на конкретные контексты и неизученные техники.

37. https://pmc.ncbi.nlm.nih.gov/articles/PMC10027281/ - Статья обзоривает аугментацию медицинских изображений, реализуя геометрические, интенсивности и GAN-методы на датасетах мозга, легких, груди, глаз. Эксперименты с ResNet101 демонстрируют точность до 88.24% с адаптированными комбинациями. Результаты определяют оптимальные техники по модальностям. Вклад включает сравнения и рекомендации для снижения переобучения.

38. https://www.mdpi.com/2076-3417/14/19/9030 - Статья применяет CTGAN, TVAE, CopulaGAN для аугментации малых опросов потребителей в маркетинге. Эксперименты на 11 датасетах показывают предсказательные приросты 8-11%, с CopulaGAN лучшим для зависимостей. Результаты улучшают точность и стабильность. Вклад усиливает анализ малых выборок и приватность.

39. https://www.mdpi.com/2079-9292/13/13/2535 - Статья показывает, что использование LLM для генерации и перефразирования текстов эффективно восполняет нехватку размеченных данных. В экспериментах на двух датасетах генерация новых примеров дала наибольший прирост точности. Авторы отмечают зависимость эффекта от качества промптов и применимости подхода в малых выборках.

40. https://www.mdpi.com/2673-2688/6/2/32 - Статья обобщает применение cGAN и других генеративных моделей для аугментации в разных областях — от медицины до спутниковых данных. Эксперименты показывают улучшение точности до 17 % при ограниченных данных. Делается вывод, что эффективность генеративной аугментации зависит от типа задачи и архитектуры сети.

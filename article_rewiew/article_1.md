В статье David Tafler (2024) «Explainable AI for Intrusion Detection Systems: Enhancing Trust and Transparency in Cybersecurity» предложен гибридный трёхслойный XAI-фреймворк для сетевых систем обнаружения вторжений (NIDS), сочетающий встроенную объяснимость через LightGBM и правило-базированные фильтры, пост-хок интерпретацию с помощью SHAP и Anchors, а также контрфактические объяснения для «что-если» анализа.

Подход включает: (1) обучение на комбинации реального трафика CIC-IDS2017 и синтетических атак, сгенерированных GAN; (2) ансамблирование LightGBM с RuleFit для извлечения читаемых правил; (3) локальные объяснения через SHAP, глобальные — через feature importance и partial dependence plots; (4) генерацию контрфактических примеров с помощью DiCE.

Результаты показывают, что предложенная модель достигает accuracy 98.9 % и F1 98.4 % — на уровне лучших «чёрных ящиков» (XGBoost, LSTM), но с fidelity объяснений 96 % и стабильностью под шумом 91 %. В эксперименте с 25 экспертами по кибербезопасности доверие к решениям системы выросло на 38 % по сравнению с baseline. Контрфактические сценарии демонстрируют, что изменение 2–3 признаков (например, уменьшение длительности потока на 40 %) переводит атаку в класс benign с вероятностью 87 %.

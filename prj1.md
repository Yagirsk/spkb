1. https://www.uni-bamberg.de/fileadmin/xai/studies/theses/2024/Master_Thesis_David_Tafler.pdf - Работа исследует потенциал генеративного увеличения данных в встраиваемом пространстве базовых моделей компьютерного зрения

2. https://arxiv.org/pdf/2502.18691 - Исследование техник аугментации (Pairwise Channel Transfer, Occlusion, Masking) для CNN на Caltech-101; + значительное улучшение точности на малых датасетах за счет предотвращения переобучения

3. https://arxiv.org/pdf/2510.20344 - Алгоритм DAERNN — аугментация для нейросетей с цензурированными данными

4. https://arxiv.org/pdf/2405.09591 - Обзор традиционных и нейронных методов аугментации данных, включая их применение в изображениях, текстах и графах для улучшения обобщения нейросетей

5. https://www.ijcai.org/proceedings/2024/0625.pdf - Метод неявной семантической аугментации для повышения устойчивости нейросетей к шумовым меткам и атакам в задачах классификации

6. https://www.techscience.com/cmc/v85n3/64189/pdf - Обзор аугментации с использованием глубоких генеративных моделей и трансфер-лернинга для малых выборок

7. https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10699340 - Обзор современных подходов к аугментации изображений для глубокого обучения, с акцентом на эффективность в компьютерном зрении

8. https://thesai.org/Downloads/Volume15No1/Paper_118-Overview_of_Data_Augmentation_Techniques.pdf - Обзор техник аугментации, включая GAN для изображений и временных рядов в гибридных CNN-LSTM моделях

9. https://aclanthology.org/2025.cl-1.6.pdf - Оценка синтетических данных из пользовательского текста для NLG-задач, включая бенчмарки и дедупликацию

10. https://arxiv.org/pdf/2402.11845 - Исследуется самообучение без меток с адаптивной аугментацией для повышения устойчивости обучения нейросетей

11. https://arxiv.org/pdf/2405.08912 - Автоматизированная курация датасетов с помощью эмбеддингов (CLIP/ViT) и кластеризации для удаления шума

12. https://ciam.ru/composites_theses/korolev.pdf?utm_source=chatgpt.com - Разработка алгоритма аугментации обучающих данных для систем компьютерного зрения


13. https://cyberleninka.ru/article/n/metod-augmentatsii-tekstovyh-dannyh-s-sohraneniem-stilya-rechi-i-leksiki-persony - Предложен метод аугментации текста, основанный на синтаксических шаблонах, сохраняющий стиль и лексикон конкретного человека. Метод протестирован на задаче определения эмоционального состояния и показал рост качества моделей для русского и английского языков.

14. https://www.mathnet.ru/links/7514bcc6dfefe622f1309a4220f46705/vkam442.pdf - исследование посвящено нейросетевой модели многомодального распознавания. В работе обсуждаются ограничения в составе обучающих множеств (наборов модальностей), что косвенно затрагивает проблему определения и формирования обучающего множества данных.

15. https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00447-5 - Обширный обзор методов аугментации изображений; включает сравнение эффективности, влияние на переобучение и выбор датасетов.

16. https://arxiv.org/pdf/2205.13445 - Исследование особенностей аугментации для Vision Transformers (ViT). Установлено, что ViT требуют более сильной аугментации (например, MixUp, CutMix) для стабильного обучения

17. https://arxiv.org/pdf/2303.10153 - Смещения фокуса с улучшения архитектур на улучшение данных. Охватывает методы аугментации, фильтрации шумных меток, балансировки и стратегии разделения данных

18. https://arxiv.org/pdf/2103.02852 - ускоренную версию AutoAugment, основанную на дифференцируемом поиске (аналогично DADA). Авторы вводят непрерывное пространство политик аугментации и оптимизируют их с помощью градиентов, что снижает вычислительные затраты на 1–2 порядка.

19. https://lup.lub.lu.se/luur/download?func=downloadFile&recordOId=9148803&fileOId=9148804 - систематический обзор методов аугментации данных в глубоком обучении, включая классические (повороты, шум) и современные (AutoAugment, MixUp, CutMix). Рассматриваются применения в компьютерном зрении, NLP и медицинской визуализации, а также влияние аугментации на обобщение и устойчивость моделей.
